# GPT-2-Playground-Train-Generate-Experiment-with-All-Parameters
An interactive Jupyter notebook to explore, generate text, and experiment with every parameter of a GPT-2 model — from temperature and top-k sampling to architecture settings like context length, number of layers, and dropout. Perfect for learning, teaching, or prototyping!
# GPT-2 Playground: Full Parameter Exploration

🎓 **Learn, Generate, and Tweak Every Aspect of GPT-2**  
This repository provides an interactive Jupyter notebook that lets you experiment with the full configuration of a GPT-2 small (124M) model — including training parameters, generation strategies, and architectural hyperparameters.

Whether you're learning how transformers work, testing text generation behavior, or prototyping ideas, this notebook is your sandbox.

👉 [Open in Google Colab](https://colab.research.google.com/github/yourusername/gpt2-playground/blob/main/gpt2_playground.ipynb) | 🚀 Run locally with Jupyter

---

## 🔍 What You Can Do

- ✅ **Text Generation** with customizable prompts
- ⚙️ **Adjust all key generation parameters**:
  - `temperature`
  - `top_k` sampling
  - `max_new_tokens`
  - `context_length`
- 🧱 **Modify model architecture**:
  - Number of layers (`n_layers`)
  - Attention heads (`n_heads`)
  - Embedding dimension (`emb_dim`)
  - Dropout rate (`drop_rate`)
  - Vocabulary size (`vocab_size`)
  - QKV bias (`qkv_bias`)
- 🔁 **Easy retraining simulation** (with synthetic data or custom datasets)
- 📈 Visualize token generation step-by-step
- 💡 Great for teaching NLP and deep learning concepts

---

## 🛠️ How to Use

Just clone the repo and run all the cells of gpt.ipynb ,change the parameters and observe how the model works.

   
