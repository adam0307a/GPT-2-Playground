# GPT-2-Playground-Train-Generate-Experiment-with-All-Parameters
An interactive Jupyter notebook to explore, generate text, and experiment with every parameter of a GPT-2 model â€” from temperature and top-k sampling to architecture settings like context length, number of layers, and dropout. Perfect for learning, teaching, or prototyping!
# GPT-2 Playground: Full Parameter Exploration

ğŸ“ **Learn, Generate, and Tweak Every Aspect of GPT-2**  
This repository provides an interactive Jupyter notebook that lets you experiment with the full configuration of a GPT-2 small (124M) model â€” including training parameters, generation strategies, and architectural hyperparameters.

Whether you're learning how transformers work, testing text generation behavior, or prototyping ideas, this notebook is your sandbox.

ğŸ‘‰ [Open in Google Colab](https://colab.research.google.com/github/yourusername/gpt2-playground/blob/main/gpt2_playground.ipynb) | ğŸš€ Run locally with Jupyter

---

## ğŸ” What You Can Do

- âœ… **Text Generation** with customizable prompts
- âš™ï¸ **Adjust all key generation parameters**:
  - `temperature`
  - `top_k` sampling
  - `max_new_tokens`
  - `context_length`
- ğŸ§± **Modify model architecture**:
  - Number of layers (`n_layers`)
  - Attention heads (`n_heads`)
  - Embedding dimension (`emb_dim`)
  - Dropout rate (`drop_rate`)
  - Vocabulary size (`vocab_size`)
  - QKV bias (`qkv_bias`)
- ğŸ” **Easy retraining simulation** (with synthetic data or custom datasets)
- ğŸ“ˆ Visualize token generation step-by-step
- ğŸ’¡ Great for teaching NLP and deep learning concepts

---

## ğŸ› ï¸ How to Use

Just clone the repo and run all the cells of gpt.ipynb ,change the parameters and observe how the model works.

   
